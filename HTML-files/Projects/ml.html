<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Projects | Magnetars</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="projects-styles.css">
</head>
<body>
    <div id="cursor"></div>

    <nav class="nav">
        <div class="nav-brand">MAGNETARS</div>
        <div class="nav-links">
            <a href="../../index.html#home">Home</a>
            <a href="../../index.html#about">About</a>
            <a href="../../index.html#work">Projects</a>
            <a href="../../index.html#publications">Publications</a>
            <a href="../../index.html#contact">Contact</a>
        </div>
    </nav>

    <div id="webgl"></div>

    <section id="project-box" class="active">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-16">
            <h1 class="text-4xl font-bold text-white text-center mb-8 py-4">Machine Learning Projects</h1>

            <!-- BPLD Project -->
            <div class="project-box">
                <div>
                    <h2 class="text-2xl font-semibold text-white mb-2">Black Gram Plant Leaf Disease Classification (BPLD)</h2>
                    <p class="text-gray-400">
                        Developed 8 machine learning models to classify diseases in black gram plant leaves using leaf images, aiding agricultural diagnostics. Models leverage deep learning and transfer learning, trained on 500+ leaf samples. A collaborative effort by Amrit Roy and Aayusha Singh.
                    </p>
                </div>
                <button onclick="openProjectModal('bpldModal')">Open Project</button>
            </div>
            
            <!-- Retina Image Enhancement Project -->
            <div class="project-box">
                <div>
                    <h2 class="text-2xl font-semibold text-white mb-2">Retina Image Enhancement</h2>
                    <p class="text-gray-400">
                        Developed 8 machine learning models to enhance retinal images for improved disease detection (e.g., diabetic retinopathy). Models include denoising, generative, and hybrid approaches, trained on 1000+ retinal images. A collaborative effort by Aayusha Singh and Amrit Roy.
                    </p>
                    <p class="text-gray-400">
                        <a href="https://github.com/aayu-sha/VAE" target="_blank" class="text-blue-400 hover:underline">GitHub Repository</a>
                    </p>
                </div>
                <button onclick="openProjectModal('retinaModal')">Open Project</button>
            </div>

            <a href="../../index.html#work" class="back-link">Back to Projects</a>
        </div>
    </section>

    <!-- Retina Project Modal -->
    <div id="retinaModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeProjectModal('retinaModal')">×</span>
            <h2 class="text-2xl font-semibold text-white mb-4">Retina Image Enhancement Models</h2>
            
            <!-- 1. Variational Autoencoder (VAE) -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/vae.png" alt="VAE Result" onclick="openImageModal('../../Projects/ML/Retina/vae.png', 'Enhanced Retinal Image using Variational Autoencoder')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">1. Variational Autoencoder (VAE)</h3>
                    <p class="text-white">
                        A generative model that learns latent representations of retinal images for enhancement, improving contrast and clarity. Reference: <a href="https://ar5iv.labs.arxiv.org/html/1606.05908" target="_blank" class="text-white hover:underline" style="color: white !important;"> Kingma & Welling, 2016 </a>
                    </p>
                    
                </div>
            </div>

            <!-- 2. Total Variational De-noising (TVD) -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/TVD.png" alt="TVD Result" onclick="openImageModal('../../Projects/ML/Retina/TVD.png', 'Denoised Retinal Image using Total Variational De-noising')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">2. Total Variational De-noising (TVD)</h3>
                    <p class="text-white">
                        Reduces noise in retinal images by minimizing total variation, preserving edges. Reference: <a href="https://www.researchgate.net/publication/222469583_Nonlinear_total_variation_based_noise_removal_algorithms" target="_blank" class="text-white hover:underline" style="color: white !important;">Rudin et al., 1992</a>.
                    </p>
                    
                </div>
            </div>

            <!-- 3. Spatial Pyramid Pooling Networks (SPP) -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/SPP.png" alt="SPP Result" onclick="openImageModal('../../Projects/ML/Retina/SPP.png', 'Enhanced Retinal Image using Spatial Pyramid Pooling Networks')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">3. Spatial Pyramid Pooling Networks (SPP)</h3>
                    <p class="text-white">
                        Uses multi-scale feature extraction to enhance retinal images, improving robustness to varying image sizes. Reference: <a href="https://arxiv.org/abs/1406.4729" target="_blank" class="text-white hover:underline" style="color: white !important;">He et al., 2014</a>.
                    </p>
                </div>
            </div>

            <!-- 4. Auto Regressive Model -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/autoregressive.png" alt="AR Result" onclick="openImageModal('../../Projects/ML/Retina/autoregressive.png', 'Enhanced Retinal Image using Auto Regressive Model')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">4. Auto Regressive Model</h3>
                    <p class="text-white">
                        Models pixel dependencies in retinal images for enhancement, focusing on statistical patterns. Reference: <a href="https://ieeexplore.ieee.org/document/1100705" target="_blank" class="text-white hover:underline" style="color: white !important;">Jain, 1981</a>.
                    </p>
                    
                </div>
            </div>

            <!-- 5. Normalizing Flows -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/normalizing.png" alt="NF Result" onclick="openImageModal('../../Projects/ML/Retina/normalizing.png', 'Enhanced Retinal Image using Normalizing Flows')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">5. Normalizing Flows</h3>
                    <p class="text-white">
                        A generative approach using invertible transformations to enhance retinal images, preserving details. Reference: <a href="http://proceedings.mlr.press/v37/rezende15.pdf" target="_blank" class="text-white hover:underline" style="color: white !important;">Rezende & Mohamed, 2015</a>.
                    </p>
                    
                </div>
            </div>

            <!-- 6. Energy Based Model (EBM) -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/ebm.png" alt="EBM Result" onclick="openImageModal('../../Projects/ML/Retina/ebm.png', 'Enhanced Retinal Image using Energy Based Model')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">6. Energy Based Model (EBM)</h3>
                    <p class="text-white">
                        Uses energy functions to model and enhance retinal images, focusing on noise reduction. Reference: <a href="https://arxiv.org/abs/1903.08689" target="_blank" class="text-white hover:underline" style="color: white !important;">Du & Mordatch, 2019</a>.
                    </p>
                    
                </div>
            </div>

            <!-- 7. Diffusion Model -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/diffusion.png" alt="Diffusion Result" onclick="openImageModal('../../Projects/ML/Retina/diffusion.png', 'Enhanced Retinal Image using Diffusion Model')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">7. Diffusion Model</h3>
                    <p class="text-white">
                        Applies iterative denoising to enhance retinal images, improving quality for medical analysis. Reference: <a href="https://arxiv.org/abs/2006.11239" target="_blank" class="text-white hover:underline" style="color: white !important;">Ho et al., 2020</a>.
                    </p>
                    
                </div>
            </div>

            <!-- 8. Hybrid Model (ALM-CNN) -->
            <div class="model-block">
                <img src="../../Projects/ML/Retina/hybrid.png" alt="ALM-CNN Result" onclick="openImageModal('../../Projects/ML/Retina/hybrid.png', 'Enhanced Retinal Image using Adaptive Local Means CNN')">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">8. Adaptive Local Means CNN (ALM-CNN)</h3>
                    <p class="text-white">
                        Combines Adaptive Local Means Denoising with a CNN to enhance retinal images, balancing noise reduction and detail preservation. A collaborative design by the team.
                    </p>
                    
                </div>
            </div>
        </div>
    </div>

    <!-- BPLD Project Modal (Placeholder) -->
    <div id="bpldModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeProjectModal('bpldModal')">×</span>
            <h2 class="text-2xl font-semibold text-white mb-4">Black Gram Plant Leaf Disease Classification Models</h2>
            
            <!-- 1. CNN -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">1. Convolutional Neural Network (CNN)</h3>
                    <p class="text-gray-400">
                        A foundational deep learning model using convolutional layers to classify leaf diseases, effective for feature extraction. Reference: Krizhevsky et al., 2012. <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Utilizes multiple convolutional layers to extract spatial features from leaf images.</li>
                        <li>Achieves high accuracy in detecting diseases like anthracnose and powdery mildew.</li>
                        <li>Trained on a dataset of 1000+ leaf images with data augmentation for robustness.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 2. DenseNet -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">2. DenseNet</h3>
                    <p class="text-gray-400">
                        Uses dense connectivity between layers to improve gradient flow and feature reuse for leaf disease classification. Reference: Huang et al., 2016. <a href="https://arxiv.org/abs/1608.06993" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Each layer receives inputs from all previous layers, enhancing feature propagation.</li>
                        <li>Reduces the number of parameters compared to traditional CNNs, improving efficiency.</li>
                        <li>Performs well on small datasets due to its dense connectivity pattern.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 3. NASNet -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">3. NASNet</h3>
                    <p class="text-gray-400">
                        Employs neural architecture search to design an efficient network for classifying leaf diseases. Reference: Zoph et al., 2016. <a href="https://arxiv.org/abs/1611.01578" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Automatically designs network architecture optimized for leaf disease classification.</li>
                        <li>Outperforms manually designed models in terms of accuracy and computational efficiency.</li>
                        <li>Utilizes a search space with recurrent neural networks to find optimal configurations.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 4. ResNeXt -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">4. ResNeXt</h3>
                    <p class="text-gray-400">
                        Introduces cardinality to ResNet, enhancing feature learning for leaf disease classification. Reference: Xie et al., 2016. <a href="https://arxiv.org/abs/1611.05431" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Improves ResNet by adding a cardinality dimension for better feature extraction.</li>
                        <li>Effective in identifying complex disease patterns in black gram leaves.</li>
                        <li>Reduces overfitting through skip connections and batch normalization.</li>
                    </ul>
                   
                </div>
            </div>

            <!-- 5. TRESNet -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">5. TRESNet</h3>
                    <p class="text-gray-400">
                        A transformer-based architecture optimized for efficient leaf disease classification. Reference: Ridnik et al., 2020. <a href="https://arxiv.org/abs/2003.13630" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Leverages transformer blocks for global feature learning in leaf images.</li>
                        <li>Optimized for speed and accuracy, suitable for real-time agricultural applications.</li>
                        <li>Reduces computational overhead while maintaining high classification accuracy.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 6. VGGNet -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">6. VGGNet</h3>
                    <p class="text-gray-400">
                        A deep convolutional network with small filters, effective for classifying leaf diseases. Reference: Simonyan & Zisserman, 2014. <a href="https://arxiv.org/abs/1409.1556" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Employs small 3x3 filters in deep layers to capture fine-grained leaf disease patterns.</li>
                        <li>High depth (16-19 layers) improves feature representation for classification.</li>
                        <li>Effective but computationally intensive due to its uniform architecture.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 7. Xception -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">7. Xception</h3>
                    <p class="text-gray-400">
                        Uses depthwise separable convolutions for efficient leaf disease classification. Reference: Chollet, 2016. <a href="https://arxiv.org/abs/1610.02357" target="_blank" class="text-blue-400 hover:underline" style="color: white !important;">Reference</a>.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Replaces standard convolutions with depthwise separable convolutions for efficiency.</li>
                        <li>Improves accuracy by focusing on cross-channel correlations in leaf images.</li>
                        <li>Reduces the number of parameters while maintaining high performance.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- 8. DRPA-Net -->
            <div class="model-block">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">8. Dense Residual Paths with Attention (DRPA-Net)</h3>
                    <p class="text-gray-400">
                        Combines DenseNet’s reuse, ResNeXt’s cardinality, and attention mechanisms, using transfer learning from pre-trained DenseNet121 for leaf disease classification. A collaborative design by the team.
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>Integrates dense connections and residual paths for enhanced feature learning.</li>
                        <li>Uses attention mechanisms to focus on disease-relevant regions in leaf images.</li>
                        <li>Leverages transfer learning from DenseNet121 for improved performance on small datasets.</li>
                        <li>Achieves state-of-the-art accuracy in black gram leaf disease classification.</li>
                    </ul>
                    
                </div>
            </div>

            <!-- Results Section -->
            <div class="model-block">
                <img src="../../Projects/ML/BPLD/photo_2025-05-18_13-11-00.jpg" alt="BPLD Results" onclick="openImageModal('../../Projects/ML/BPLD/photo_2025-05-18_13-11-00.jpg', 'Results of Black Gram Plant Leaf Disease Classification Models')" style="width: 150px; height: 150px; border-radius: 8px; cursor: pointer; transition: transform 0.3s ease;" onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1.0)'">
                <div class="model-details">
                    <h3 class="text-xl font-semibold text-white">Results</h3>
                    <p class="text-gray-400">
                        The 8 models were evaluated on a test set of 150 black gram leaf images, achieving the following accuracies:
                    </p>
                    <ul class="text-gray-400 list-disc list-inside mt-2">
                        <li>CNN: 85.3% accuracy, effective for basic disease classification.</li>
                        <li>DenseNet: 89.7% accuracy, benefiting from dense connectivity.</li>
                        <li>NASNet: 87.5% accuracy, optimized architecture via neural search.</li>
                        <li>ResNeXt: 90.2% accuracy, improved feature learning with cardinality.</li>
                        <li>TRESNet: 91.0% accuracy, efficient transformer-based approach.</li>
                        <li>VGGNet: 86.8% accuracy, strong but computationally heavy.</li>
                        <li>Xception: 88.4% accuracy, efficient with depthwise convolutions.</li>
                        <li>DRPA-Net: 93.5% accuracy, outperforming others with attention mechanisms and transfer learning.</li>
                    </ul>
                </div>
            </div>
        </div>
        
    </div>

    <!-- Image Zoom Modal -->
    <div id="imageModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeImageModal()">×</span>
            <img id="modalImage" src="" alt="" class="w-full rounded-lg">
            <p id="modalCaption" class="text-center text-gray-400 mt-4"></p>
        </div>
    </div>

    <div id="imageModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeImageModal()">×</span>
            <img id="modalImage" src="" alt="" class="rounded-lg">
            <p id="modalCaption" class="text-center text-gray-400 mt-4"></p>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.4/gsap.min.js"></script>
    <script src="../../publication-script.js"></script>
    <script>
        // GSAP Animation for project boxes
        document.addEventListener('DOMContentLoaded', () => {
            gsap.from('.project-box', {
                opacity: 0,
                y: 30,
                duration: 1,
                stagger: 0.2,
                ease: 'power3.out'
            });
        });

        // Project Modal functions
        function openProjectModal(modalId) {
            const modal = document.getElementById(modalId);
            modal.style.display = 'block';
            gsap.from(modal.querySelectorAll('.model-block'), {
                opacity: 0,
                y: 20,
                duration: 0.5,
                stagger: 0.1,
                ease: 'power3.out'
            });
        }

        function closeProjectModal(modalId) {
            document.getElementById(modalId).style.display = 'none';
        }

        // Image Modal functions
        function openImageModal(src, caption) {
            const modal = document.getElementById('imageModal');
            const modalImage = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            modalImage.src = src;
            modalCaption.textContent = caption;
            modal.style.display = 'block';
        }

        function closeImageModal() {
            document.getElementById('imageModal').style.display = 'none';
        }

        // Close modals when clicking outside or pressing Esc
        window.onclick = function(event) {
            const retinaModal = document.getElementById('retinaModal');
            const bpldModal = document.getElementById('bpldModal');
            const imageModal = document.getElementById('imageModal');
            if (event.target == retinaModal) closeProjectModal('retinaModal');
            if (event.target == bpldModal) closeProjectModal('bpldModal');
            if (event.target == imageModal) closeImageModal();
        };

        document.addEventListener('keydown', (event) => {
            if (event.key === 'Escape') {
                closeProjectModal('retinaModal');
                closeProjectModal('bpldModal');
                closeImageModal();
            }
        });
    </script>
    <script src="projects.js"></script>
</body>
</html>